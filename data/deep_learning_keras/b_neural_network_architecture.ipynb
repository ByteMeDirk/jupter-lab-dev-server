{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34c1d33-52bd-4504-9df9-01fcd5169605",
   "metadata": {},
   "source": [
    "# The Input Layer\n",
    "\n",
    "## Vectors\n",
    "\n",
    "In the context of an Artificial Neural Network (ANN), vectors are a type of data structure that are used to represent the input data that is fed into the network. \n",
    "\n",
    "A vector is essentially a list of numerical values. Each value in the vector represents a different feature of the input data. For example, in an image recognition task, a vector might contain the pixel values for a single image. In a text classification task, a vector might contain the frequency of certain words or phrases.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example of an np array\n",
    "input_vector = np.array([0.5, 0.75, 0.25])\n",
    "```\n",
    "\n",
    "The input layer of an ANN takes in one or more of these vectors. The number of nodes in the input layer is typically equal to the number of features in the input vector. Each node in the input layer corresponds to a different feature, and the value of that node is set to the value of that feature in the input vector.\n",
    "\n",
    "For example, if we have an input vector `[0.5, 0.75, 0.25]`, and an ANN with an input layer consisting of three nodes, the first node would take the value `0.5`, the second node would take the value `0.75`, and the third node would take the value `0.25`.\n",
    "\n",
    "This is how the ANN takes in the input data and begins the process of forward propagation, which eventually leads to a prediction output by the network.\n",
    "\n",
    "## Input Preprocessing\n",
    "\n",
    "Input preprocessing is a crucial step in preparing your data for an Artificial Neural Network (ANN). It involves transforming raw data into an understandable format for your ANN. Here are some common preprocessing steps:\n",
    "\n",
    "1. **Normalization**: This is the process of scaling the input features to a certain range (usually between 0 and 1 or -1 and 1). This is important because features in different scales can impact the model's ability to learn from the data effectively. In Python, you can use libraries like `numpy` or `sklearn` to normalize your data.\n",
    "\n",
    "2. **One-hot Encoding**: This is used when dealing with categorical data. It involves converting each categorical value into a new categorical column and assigns a binary value of 1 or 0. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1.\n",
    "\n",
    "3. **Handling Missing Values**: If your dataset has missing values, you'll need to handle them before feeding the data into your ANN. You could remove the rows with missing data, but this could result in losing valuable data. Another way is to impute the missing values with the mean, median, or mode.\n",
    "\n",
    "Here's an example of how you might preprocess your input data in Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Assuming X is your input data\n",
    "X = np.array([[0.5, 0.75, 0.25], [0.1, 0.6, 0.4], [0.3, 0.8, 0.5]])\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "# One-hot encode the data (assuming it's categorical)\n",
    "# For this example, let's assume the second feature is categorical with values 0.75, 0.6, and 0.8\n",
    "one_hot = preprocessing.OneHotEncoder()\n",
    "one_hot.fit(X[:, 1].reshape(-1, 1))\n",
    "X_one_hot = one_hot.transform(X[:, 1].reshape(-1, 1)).toarray()\n",
    "\n",
    "# Replace the second feature in X_normalized with the one-hot encoded data\n",
    "X_normalized = np.delete(X_normalized, 1, 1)  # delete second column from X_normalized\n",
    "X_preprocessed = np.hstack((X_normalized, X_one_hot))  # add one-hot encoded data\n",
    "\n",
    "print(X_preprocessed)\n",
    "```\n",
    "\n",
    "Input preprocessing in machine learning depends on the type of data you're dealing with. Here are some common types of data and the preprocessing techniques used for each:\n",
    "\n",
    "1. **Numerical Data**: This type of data is quantitative and can be discrete (like the number of rooms in a house) or continuous (like temperature). Preprocessing techniques for numerical data include:\n",
    "   - Normalization: Scales the data to a certain range, usually between 0 and 1.\n",
    "   - Standardization: Scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Categorical Data**: This type of data represents characteristics such as a person's gender, marital status, hometown, etc. Preprocessing techniques for categorical data include:\n",
    "   - One-Hot Encoding: Converts each category value into a new column and assigns a 1 or 0 (True/False) value to the column.\n",
    "   - Label Encoding: Converts each value in a column to a number. Good for ordinal data (data that has an order to it) like high, medium, and low.\n",
    "\n",
    "3. **Text Data**: This type of data is unstructured and needs to be converted into numerical form for machine learning. Preprocessing techniques for text data include:\n",
    "   - Bag of Words: Represents text data in terms of a \"bag\" (multiset) of its words, disregarding grammar and word order but keeping multiplicity.\n",
    "   - TF-IDF: Stands for Term Frequency-Inverse Document Frequency. It's a numerical statistic used to reflect how important a word is to a document in a collection or corpus.\n",
    "   - Word Embedding (like Word2Vec): Represents words in a coordinate system where related words appear in close proximity to each other.\n",
    "\n",
    "4. **Image Data**: This type of data requires different preprocessing techniques, including:\n",
    "   - Rescaling: Images are matrices of pixel intensities, usually ranging from 0 to 255. Rescaling such images to a scale between 0 and 1 helps the model learn more effectively.\n",
    "   - Normalization: Similar to rescaling, but here the image data is adjusted to have a mean of 0 and standard deviation of 1.\n",
    "   - Data Augmentation: Techniques like rotation, zooming, flipping, etc., can help to expand the dataset and improve the model's performance.\n",
    "\n",
    "Remember, the preprocessing steps you choose to implement will depend on the nature of your data and the specific requirements of your task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d3cc6-469a-415a-9c75-b9f93f26f624",
   "metadata": {},
   "source": [
    "# The Hidden layer\n",
    "\n",
    "In an Artificial Neural Network (ANN), the hidden layers are the layers between the input layer and the output layer. They are called \"hidden\" because they are not directly observable from the network's input or output.\n",
    "\n",
    "Here's a more detailed explanation:\n",
    "\n",
    "1. **Nodes**: Each hidden layer consists of one or more nodes (or neurons). Each node takes in some input, applies a weight to it, adds a bias, and then passes the result through an activation function. The output is then passed on as input to the nodes in the next layer.\n",
    "\n",
    "2. **Weights and Biases**: The weights and biases in the nodes of the hidden layers are the parameters that the network learns during the training process. The weights determine the importance of the input value, and the bias allows the activation function to be shifted to the left or right to better fit the data.\n",
    "\n",
    "3. **Activation Function**: The activation function determines the output of a node given an input or set of inputs. Common choices for activation functions include the sigmoid function, the hyperbolic tangent function, and the ReLU (Rectified Linear Unit) function.\n",
    "\n",
    "4. **Depth and Width**: The number of hidden layers in a network is referred to as its depth, and the number of nodes in each hidden layer is referred to as its width. The depth and width of the network are hyperparameters that you can tune to find the best fit for your data.\n",
    "\n",
    "5. **Function**: The hidden layers are responsible for learning and representing the complex patterns in the data. Each layer captures and represents different levels of abstraction of the input data.\n",
    "\n",
    "Here's a simple representation of a hidden layer in a neural network:\n",
    "\n",
    "```python\n",
    "class HiddenLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.biases = np.random.rand(output_size)\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.outputs\n",
    "```\n",
    "\n",
    "In this code, `input_size` is the number of nodes in the previous layer (or the number of features in the input data for the first hidden layer), and `output_size` is the number of nodes in the current hidden layer. The `forward_propagation` method calculates the output of the layer given some inputs. Note that this code does not include an activation function, which you would typically apply to `self.outputs` before returning it.\n",
    "\n",
    "## Determining the Hidden Layer Architecture\n",
    "\n",
    "Determining the architecture of the hidden layers in an Artificial Neural Network (ANN) is more of an art than a science. There are no hard and fast rules, but there are some general guidelines that can help:\n",
    "\n",
    "1. **Number of Hidden Layers**: For many problems, you can start with one or two hidden layers and it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to overfit. Overfitting happens when your model learns the training data too well and performs poorly on unseen data.\n",
    "\n",
    "2. **Number of Neurons in Hidden Layers**: The number of neurons in the hidden layers is usually set between the size of the input layer and the size of the output layer. A common practice is to choose a number of neurons that forms a kind of pyramid, with fewer neurons in each successive layer. For example, if you have 10 input neurons and 2 output neurons, you might have 6 neurons in the first hidden layer and 4 in the second.\n",
    "\n",
    "3. **Overfitting and Underfitting**: If your network is overfitting, you can try reducing the number of hidden layers and/or the number of neurons in each layer. If your network is underfitting, you can try adding more layers and/or neurons.\n",
    "\n",
    "4. **Validation Performance**: The ultimate measure of your network's architecture is how well it performs on validation data. You can use techniques like cross-validation to estimate your network's performance on unseen data.\n",
    "\n",
    "5. **Experimentation**: Machine learning involves a lot of trial and error. Don't be afraid to experiment with different architectures to see what works best for your specific problem.\n",
    "\n",
    "Remember, these are just guidelines. The optimal architecture for an ANN depends heavily on the specific problem and the specific dataset. It's often a good idea to try out several different architectures and see which one performs best on your validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d7a89-9a9d-4db1-bc8a-0cfbffb67d5d",
   "metadata": {},
   "source": [
    "# Weights & Biases\n",
    "\n",
    "In an Artificial Neural Network (ANN), weights and biases are two of the main components that the network learns during the training process. They are parameters of the model that are adjusted through the learning process to minimize the error in the network's predictions.\n",
    "\n",
    "**Weights**: Weights are the coefficients that the network learns to adjust the importance of input features. Each input node (or neuron) in a layer is connected to each output node in the next layer through a connection that has a weight associated with it. These weights are applied to the input data and determine how much influence the input will have on the output. For example, if the weight is large, then a small change in input will result in a large change in output.\n",
    "\n",
    "**Biases**: Biases are another type of coefficient that the network learns, which are added to the weighted input to form the net input of a neuron. The bias allows the activation function to be shifted to the left or right, which can help the neuron model complex patterns. For example, if all input features are zero, the output of the neuron is equal to the bias.\n",
    "\n",
    "## Computing Weights & Biases\n",
    "\n",
    "Computing weights and biases for a three-input, four-layer Artificial Neural Network (ANN) involves initializing these parameters and then updating them through a process called backpropagation during the training phase.\n",
    "\n",
    "Here's a simplified example of how you might initialize and update these parameters in Python using a basic implementation of an ANN. Note that this is a very simplified version of an ANN and doesn't include important aspects like activation functions or a proper backpropagation algorithm.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_weights_and_biases()\n",
    "\n",
    "    def initialize_weights_and_biases(self):\n",
    "        layers = [self.input_size] + self.hidden_layers + [self.output_size]\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.weights.append(np.random.rand(layers[i], layers[i+1]))\n",
    "            self.biases.append(np.random.rand(layers[i+1]))\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        current_output = inputs\n",
    "        for i in range(len(self.weights)):\n",
    "            current_output = np.dot(current_output, self.weights[i]) + self.biases[i]\n",
    "        return current_output\n",
    "\n",
    "    # This is a placeholder for the backpropagation process\n",
    "    # In a real scenario, you would use a proper backpropagation algorithm here\n",
    "    def backpropagation(self, inputs, target_output):\n",
    "        pass\n",
    "\n",
    "    def train(self, inputs, target_output):\n",
    "        self.forward_propagation(inputs)\n",
    "        self.backpropagation(inputs, target_output)\n",
    "\n",
    "# Initialize a neural network with 3 inputs, 4 hidden layers of 5 neurons each, and 1 output\n",
    "nn = NeuralNetwork(3, [5, 5, 5, 5], 1)\n",
    "\n",
    "# Print initial weights and biases\n",
    "print(\"Initial weights: \", nn.weights)\n",
    "print(\"Initial biases: \", nn.biases)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02b2aa-e3b4-43a1-ad40-d00932c77a5d",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Activation functions in an Artificial Neural Network (ANN) play a crucial role in determining what information gets passed on to the next layer. They are applied to the output of each node in a layer, and the result is used as input to the nodes in the next layer. Here are some common types of activation functions:\n",
    "\n",
    "1. **Sigmoid Function**: The sigmoid function takes any range real number and returns the output value which falls in the range of 0 to 1. It is useful in the output layer of a binary classification, where we need probabilities that sum up to one.\n",
    "\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "2. **ReLU (Rectified Linear Unit) Function**: The ReLU function gives an output x if x is positive and 0 otherwise. It is the most widely used activation function in deep learning models. The function and its derivative both are monotonic.\n",
    "\n",
    "```python\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "```\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent) Function**: The tanh function is similar to the sigmoid function but better. The range of the tanh function is from (-1 to 1). Tanh is also sigmoidal (s - shaped).\n",
    "\n",
    "```python\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "```\n",
    "\n",
    "4. **Softmax Function**: The softmax function is often used in the output layer of a neural network. It can handle multiple classes and is useful in cases where we need probabilities for multiple classes in multiclass classification, as the probabilities sum up to one.\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "```\n",
    "\n",
    "These activation functions introduce non-linear properties to the network, which allow it to learn from the error and adjust the weights and biases during backpropagation. They also help to normalize the output of each neuron to a range between 1 and 0 or between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd65813-9314-4962-9ae1-dfcb95313b4d",
   "metadata": {},
   "source": [
    "# Output Layer\n",
    "\n",
    "The output layer in an Artificial Neural Network (ANN) is the final layer that produces the results for given inputs. The output layer processes the values from the last hidden layer and transforms them into the output format suitable for your problem. The number of nodes in the output layer depends on the type of problem you are trying to solve. For example, for binary classification, there would be one output node, for multi-class classification, the number of output nodes would be equal to the number of classes, and for regression problems, the number of output nodes would typically be one.\n",
    "\n",
    "Like the hidden layers, the output layer also has its own weights and biases. Each node in the output layer has a weight associated with it for every node in the last hidden layer. These weights are used to adjust the importance of the outputs from the last hidden layer. The biases in the output layer allow the activation function to be shifted to the left or right to better fit the data.\n",
    "\n",
    "The output layer also uses an activation function to transform its inputs into its outputs. The choice of activation function in the output layer depends on the type of problem. For binary classification problems, the sigmoid function is often used. For multi-class classification problems, the softmax function is typically used. For regression problems, no activation function might be used, or a linear activation function might be used.\n",
    "\n",
    "Here's a simple representation of an output layer in a neural network:\n",
    "\n",
    "```python\n",
    "class OutputLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.biases = np.random.rand(output_size)\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.outputs\n",
    "```\n",
    "\n",
    "In this code, `input_size` is the number of nodes in the last hidden layer, and `output_size` is the number of nodes in the output layer. The `forward_propagation` method calculates the output of the layer given some inputs. Note that this code does not include an activation function, which you would typically apply to `self.outputs` before returning it. The choice of activation function would depend on the type of problem you are trying to solve.\n",
    "\n",
    "## Determining the Output Layer Size\n",
    "\n",
    "The output layer in an Artificial Neural Network (ANN) is the final layer that produces the results for given inputs. The output layer processes the values from the last hidden layer and transforms them into the output format suitable for your problem. The number of nodes in the output layer depends on the type of problem you are trying to solve. For example, for binary classification, there would be one output node, for multi-class classification, the number of output nodes would be equal to the number of classes, and for regression problems, the number of output nodes would typically be one.\n",
    "\n",
    "Like the hidden layers, the output layer also has its own weights and biases. Each node in the output layer has a weight associated with it for every node in the last hidden layer. These weights are used to adjust the importance of the outputs from the last hidden layer. The biases in the output layer allow the activation function to be shifted to the left or right to better fit the data.\n",
    "\n",
    "The output layer also uses an activation function to transform its inputs into its outputs. The choice of activation function in the output layer depends on the type of problem. For binary classification problems, the sigmoid function is often used. For multi-class classification problems, the softmax function is typically used. For regression problems, no activation function might be used, or a linear activation function might be used.\n",
    "\n",
    "Here's a simple representation of an output layer in a neural network:\n",
    "\n",
    "```python\n",
    "class OutputLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.biases = np.random.rand(output_size)\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.outputs\n",
    "```\n",
    "\n",
    "In this code, `input_size` is the number of nodes in the last hidden layer, and `output_size` is the number of nodes in the output layer. The `forward_propagation` method calculates the output of the layer given some inputs. Note that this code does not include an activation function, which you would typically apply to `self.outputs` before returning it. The choice of activation function would depend on the type of problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39fa19-6880-4cfa-8bdb-8f654b7a3769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
