{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18fbe8a-a150-4542-89b3-a80c8c2b2323",
   "metadata": {},
   "source": [
    "# Setup & Initialization\n",
    "\n",
    "## Input Preprocessing\n",
    "\n",
    "Input preprocessing is a crucial step in preparing your data for an Artificial Neural Network (ANN). It involves transforming raw data into an understandable format for your ANN. Here are some common preprocessing steps:\n",
    "\n",
    "1. **Normalization**: This is the process of scaling the input features to a certain range (usually between 0 and 1). This is important because features in different scales can impact the model's ability to learn from the data effectively. In Python, you can use libraries like `numpy` or `sklearn` to normalize your data.\n",
    "\n",
    "2. **One-hot Encoding**: This is used when dealing with categorical data. It involves converting each categorical value into a new categorical column and assigns a binary value of 1 or 0. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1.\n",
    "\n",
    "3. **Handling Missing Values**: If your dataset has missing values, you'll need to handle them before feeding the data into your ANN. You could remove the rows with missing data, but this could result in losing valuable data. Another way is to impute the missing values with the mean, median, or mode.\n",
    "\n",
    "Here's an example of how you might preprocess your input data in Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Assuming X is your input data\n",
    "X = np.array([[0.5, 0.75, 0.25], [0.1, 0.6, 0.4], [0.3, 0.8, 0.5]])\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "# One-hot encode the data (assuming it's categorical)\n",
    "# For this example, let's assume the second feature is categorical with values 0.75, 0.6, and 0.8\n",
    "one_hot = preprocessing.OneHotEncoder()\n",
    "one_hot.fit(X[:, 1].reshape(-1, 1))\n",
    "X_one_hot = one_hot.transform(X[:, 1].reshape(-1, 1)).toarray()\n",
    "\n",
    "# Replace the second feature in X_normalized with the one-hot encoded data\n",
    "X_normalized = np.delete(X_normalized, 1, 1)  # delete second column from X_normalized\n",
    "X_preprocessed = np.hstack((X_normalized, X_one_hot))  # add one-hot encoded data\n",
    "\n",
    "print(X_preprocessed)\n",
    "```\n",
    "\n",
    "In this example, we first normalize the data using the `normalize` function from `sklearn.preprocessing`. Then we one-hot encode the second feature using the `OneHotEncoder` class from `sklearn.preprocessing`. Finally, we replace the second feature in the normalized data with the one-hot encoded data. The result is our preprocessed input data.\n",
    "\n",
    "## Splitting Input\n",
    "\n",
    "Splitting your data into training, validation, and test sets is a crucial step in preparing your data for an Artificial Neural Network (ANN). The training set is used to train the model, the validation set is used to tune the model's hyperparameters and prevent overfitting, and the test set is used to evaluate the model's performance on unseen data.\n",
    "\n",
    "Here's how you might split your data into training, validation, and test sets in Python using the `train_test_split` function from `sklearn.model_selection`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your input data and y are your targets\n",
    "# First, split the data into a training set and a temporary set using an 80-20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split the temporary set into a validation set and a test set using a 50-50 split\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "```\n",
    "\n",
    "In this code, `X` is your input data and `y` are your targets. The `train_test_split` function splits `X` and `y` into a training set and a temporary set. The training set contains 80% of the original data, and the temporary set contains the remaining 20%. Then, the temporary set is split into a validation set and a test set, each containing 50% of the temporary set (or 10% of the original data).\n",
    "\n",
    "The `random_state` parameter is used to ensure that the splits you generate are reproducible. If you run the code again with the same value of `random_state`, you will get the same splits.\n",
    "\n",
    "After running this code, you would use `X_train` and `y_train` to train your model, `X_val` and `y_val` to validate your model, and `X_test` and `y_test` to test your model.\n",
    "\n",
    "## Model Architecture & Hyper Parameters\n",
    "\n",
    "Artificial Neural Network (ANN) model architecture and hyperparameters are crucial aspects of building a successful ANN. The architecture refers to the structure of the network, including the number of layers, the number of nodes in each layer, and the activation functions used. Hyperparameters are parameters whose values are set before the learning process begins and are not learned from the data.\n",
    "\n",
    "1. **Model Architecture**:\n",
    "\n",
    "   - **Layers**: The number of layers in an ANN is a key aspect of its architecture. A basic ANN consists of an input layer, one or more hidden layers, and an output layer. The input layer corresponds to the features in your data, the hidden layers learn representations of the data, and the output layer makes the final prediction. The number of hidden layers typically depends on the complexity of the problem - more complex problems may require more layers.\n",
    "\n",
    "   - **Nodes**: The number of nodes in each layer is another important aspect. The input layer typically has a number of nodes equal to the number of features in your data. The number of nodes in the hidden layers and the output layer can vary. A common strategy is to start with a larger number of nodes in the first hidden layer and gradually decrease the number in subsequent layers.\n",
    "\n",
    "   - **Activation Functions**: Activation functions introduce non-linearity into the network, allowing it to learn more complex patterns. Common activation functions include the sigmoid function, the hyperbolic tangent function, and the ReLU (Rectified Linear Unit) function. The choice of activation function can depend on the type of problem - for example, the sigmoid function is often used for binary classification problems, while the softmax function is used for multi-class classification problems.\n",
    "\n",
    "2. **Hyperparameters**:\n",
    "\n",
    "   - **Learning Rate**: This is a key hyperparameter that determines how much the weights in the network are adjusted during each step of the learning process. A high learning rate can cause the learning process to converge quickly, but it may also overshoot the optimal solution. A low learning rate can lead to more precise learning, but it may also cause the process to converge slowly.\n",
    "\n",
    "   - **Batch Size**: This is the number of training examples used in one iteration of model training. Smaller batch sizes can lead to more precise updates, but they can also be slower and less stable. Larger batch sizes can lead to faster and more stable learning, but they may also miss some features in the data.\n",
    "\n",
    "   - **Number of Epochs**: This is the number of times the learning algorithm will work through the entire training dataset. Too few epochs can result in underfitting of the model, while too many epochs can lead to overfitting.\n",
    "\n",
    "   - **Regularization**: Regularization techniques, such as L1 and L2 regularization, can be used to prevent overfitting by adding a penalty to the loss function based on the size of the weights.\n",
    "\n",
    "The selection of architecture and hyperparameters can depend on many factors, including the complexity of the problem, the size and nature of the data, and computational constraints. It often involves a process of trial and error, and techniques such as cross-validation and grid search can be used to systematically find good values.\n",
    "\n",
    "## Weights Initialization\n",
    "\n",
    "Before training begins, the weights in an Artificial Neural Network (ANN) are typically initialized with small random values. This is done because if all the weights were initialized with the same value, all the neurons in the network would learn the same features during training, which would defeat the purpose of having multiple neurons.\n",
    "\n",
    "There are several methods for initializing the weights in an ANN:\n",
    "\n",
    "1. **Zero Initialization**: This is the simplest method, where all the weights are initialized to zero. However, this method is not recommended because all neurons will evolve symmetrically during training, meaning they'll learn the same features from the input data.\n",
    "\n",
    "2. **Random Initialization**: In this method, weights are initialized with small random numbers. This breaks the symmetry and allows the neurons to learn different features from the input data. However, the distribution of the random numbers can have an impact on the learning process.\n",
    "\n",
    "3. **Xavier/Glorot Initialization**: This method takes into account the size of the previous layer in the network. The weights are drawn from a distribution with zero mean and a specific variance (1/number of input nodes).\n",
    "\n",
    "4. **He Initialization**: This method is similar to Xavier initialization, but it's designed for layers that use the ReLU activation function. The variance of the distribution is set to (2/number of input nodes).\n",
    "\n",
    "Here's an example of how you might initialize the weights in a layer of an ANN in Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, initialization='random'):\n",
    "        if initialization == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif initialization == 'random':\n",
    "            self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        elif initialization == 'xavier':\n",
    "            self.weights = np.random.randn(input_size, output_size) * np.sqrt(1. / input_size)\n",
    "        elif initialization == 'he':\n",
    "            self.weights = np.random.randn(input_size, output_size) * np.sqrt(2. / input_size)\n",
    "        else:\n",
    "            raise ValueError('Invalid initialization method')\n",
    "\n",
    "        self.biases = np.zeros(output_size)\n",
    "```\n",
    "\n",
    "In this code, `input_size` is the number of nodes in the previous layer, and `output_size` is the number of nodes in the current layer. The `initialization` parameter determines the method used to initialize the weights. The biases are always initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bde57-27ba-438b-910b-27b0146f9153",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "Forward propagation is the process by which an Artificial Neural Network (ANN) makes its predictions. It involves passing the input data through each layer of the network, from the input layer to the output layer, and calculating the output of each node along the way.\n",
    "\n",
    "Here's a step-by-step explanation of how forward propagation works:\n",
    "\n",
    "1. **Input Layer**: The process begins at the input layer, where each node takes an element of the input data. The number of nodes in the input layer is typically equal to the number of features in the input data.\n",
    "\n",
    "2. **Hidden Layers**: The data then moves through the hidden layers. Each node in a hidden layer takes the outputs of all the nodes in the previous layer, multiplies each one by the corresponding weight, and adds them all together. This sum is then passed through an activation function, and the result is the output of the node.\n",
    "\n",
    "3. **Output Layer**: The process is similar in the output layer. Each node takes the outputs of all the nodes in the last hidden layer, multiplies each one by the corresponding weight, and adds them all together. This sum is then passed through an activation function. The outputs of the nodes in the output layer are the final outputs of the network.\n",
    "\n",
    "4. **Prediction**: The final outputs of the network are its predictions. The interpretation of these predictions depends on the type of problem. For a binary classification problem, there would be one output node, and its output would be interpreted as the probability of the positive class. For a multi-class classification problem, there would be one output node for each class, and the outputs would be interpreted as the probabilities of each class. For a regression problem, there would typically be one output node, and its output would be the prediction itself.\n",
    "\n",
    "5. **Actual Outputs**: The actual outputs are the true values for the target variable in your data. These are the values that you want your network to predict. During the training process, the network's predictions are compared to the actual outputs, and the difference (the error) is used to update the weights and biases in the network.\n",
    "\n",
    "Here's an example of how you might implement forward propagation in Python:\n",
    "\n",
    "```python\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        current_output = inputs\n",
    "        for layer in self.layers:\n",
    "            current_output = layer.forward_propagation(current_output)\n",
    "        return current_output\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_function):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.biases = np.random.rand(output_size)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        z = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.activation_function(z)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initialize a neural network with 1 input layer, 2 hidden layers, and 1 output layer\n",
    "nn = NeuralNetwork([\n",
    "    Layer(3, 5, sigmoid),  # input layer\n",
    "    Layer(5, 5, sigmoid),  # hidden layer 1\n",
    "    Layer(5, 5, sigmoid),  # hidden layer 2\n",
    "    Layer(5, 1, sigmoid)   # output layer\n",
    "])\n",
    "\n",
    "# Example input\n",
    "inputs = np.array([0.5, 0.75, 0.25])\n",
    "\n",
    "# Forward propagation\n",
    "predictions = nn.forward_propagation(inputs)\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "In this code, `NeuralNetwork` is a class representing an ANN, and `Layer` is a class representing a layer in the network. The `forward_propagation` method in the `NeuralNetwork` class passes the input data through each layer in the network, and the `forward_propagation` method in the `Layer` class calculates the output of the layer given some inputs. The `sigmoid` function is used as the activation function in all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d302e-1001-4927-a65d-f0070b840350",
   "metadata": {},
   "source": [
    "# Measuring Accuracy & Error\n",
    "\n",
    "In an Artificial Neural Network (ANN), the accuracy of the model's predictions is measured by comparing the predicted outputs (often denoted as y-hat) with the actual outputs (often denoted as y). The difference between these two values is often referred to as the error.\n",
    "\n",
    "There are several ways to measure this error, and the choice of method can depend on the type of problem you are trying to solve. Here are some common methods:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: This is often used in regression problems. It calculates the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "```python\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "```\n",
    "\n",
    "2. **Cross-Entropy Loss**: This is often used in classification problems. It calculates the negative log likelihood of the actual class labels given the predicted probabilities.\n",
    "\n",
    "```python\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "```\n",
    "\n",
    "3. **Binary Cross-Entropy Loss**: This is a special case of cross-entropy loss that is used for binary classification problems.\n",
    "\n",
    "```python\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "```\n",
    "\n",
    "The overall goal of training an ANN is to minimize this error. This is done through a process called backpropagation, which involves adjusting the weights and biases in the network in a way that minimally decreases the error.\n",
    "\n",
    "The error for a single training example is often referred to as the loss, and the average error over the entire training set is often referred to as the cost. The cost function is what the network aims to minimize during the training process.\n",
    "\n",
    "Here's an example of how you might calculate the cost in Python:\n",
    "\n",
    "```python\n",
    "def compute_cost(y_true, y_pred):\n",
    "    m = y_true.shape[0]  # number of examples\n",
    "    cost = (1/m) * np.sum(np.square(y_pred - y_true))\n",
    "    return cost\n",
    "```\n",
    "\n",
    "In this code, `y_true` is the actual outputs and `y_pred` is the predicted outputs. The `compute_cost` function calculates the mean squared error between these two values, which is the cost that the network aims to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebabc5-f92f-4ea3-b314-4ff5d5f7f586",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "Backpropagation is a key algorithm used in training an Artificial Neural Network (ANN). It's a type of supervised learning method that uses gradient descent to adjust the weights and biases in the network to minimize the error in the network's predictions.\n",
    "\n",
    "Here's a step-by-step explanation of how backpropagation works:\n",
    "\n",
    "1. **Forward Propagation**: The process begins with a forward pass through the network, where the input data is passed through each layer from the input layer to the output layer, and the output of each node is calculated along the way. This results in a predicted output.\n",
    "\n",
    "2. **Compute Loss**: The error (or loss) of the network is then calculated by comparing the predicted output to the actual output. The loss function used to calculate the error depends on the type of problem (e.g., regression, binary classification, multi-class classification).\n",
    "\n",
    "3. **Backward Propagation**: The error is then propagated back through the network, from the output layer to the input layer. The gradient of the loss function with respect to each weight and bias in the network is calculated along the way. This gradient indicates how much the loss would change if the corresponding weight or bias were changed by a small amount.\n",
    "\n",
    "4. **Update Weights and Biases**: The weights and biases in the network are then updated in the opposite direction of the gradients. This is done to decrease the loss. The size of the updates is determined by the learning rate, which is a hyperparameter that controls how fast the network learns.\n",
    "\n",
    "5. **Iterate**: This process is repeated for a number of iterations (or epochs). With each iteration, the weights and biases in the network are adjusted to gradually decrease the loss, and the network becomes better at predicting the output for given inputs.\n",
    "\n",
    "Here's a simplified example of how you might implement backpropagation in Python:\n",
    "\n",
    "```python\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        current_output = inputs\n",
    "        for layer in self.layers:\n",
    "            current_output = layer.forward_propagation(current_output)\n",
    "        return current_output\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return np.mean(np.square(y_true - y_pred))  # Mean Squared Error loss\n",
    "\n",
    "    def back_propagation(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        for layer in reversed(self.layers):\n",
    "            error = layer.back_propagation(error)\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.weights += learning_rate * layer.d_weights\n",
    "            layer.biases += learning_rate * layer.d_biases\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            y_pred = self.forward_propagation(X)\n",
    "            self.back_propagation(y, y_pred)\n",
    "            self.update_weights(learning_rate)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.biases = np.random.rand(output_size)\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.outputs\n",
    "\n",
    "    def back_propagation(self, error):\n",
    "        self.d_weights = np.dot(self.inputs.T, error)\n",
    "        self.d_biases = np.sum(error, axis=0, keepdims=True)\n",
    "        return np.dot(error, self.weights.T)\n",
    "```\n",
    "\n",
    "In this code, `NeuralNetwork` is a class representing an ANN, and `Layer` is a class representing a layer in the network. The `forward_propagation` method in the `NeuralNetwork` class passes the input data through each layer in the network, and the `forward_propagation` method in the `Layer` class calculates the output of the layer given some inputs. The `back_propagation` method in the `NeuralNetwork` class propagates the error back through the network, and the `back_propagation` method in the `Layer` class calculates the gradients of the weights and biases in the layer. The `update_weights` method in the `NeuralNetwork` class updates the weights and biases in each layer according to the gradients and the learning rate. The `train` method in the `NeuralNetwork` class performs the training process for a given number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7e643-559d-46b7-9557-b02d0a9fd100",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm that's used to learn the weight and bias parameters in a neural network. It's used during the training phase of the neural network where we aim to minimize the cost function.\n",
    "\n",
    "Here's a step-by-step explanation of how Gradient Descent works in the context of a neural network:\n",
    "\n",
    "1. **Forward Propagation**: The process begins with a forward pass through the network. The input data is passed through each layer from the input layer to the output layer, and the output of each node is calculated along the way. This results in a predicted output.\n",
    "\n",
    "2. **Compute Loss**: The error (or loss) of the network is then calculated by comparing the predicted output to the actual output. The loss function used to calculate the error depends on the type of problem (e.g., regression, binary classification, multi-class classification).\n",
    "\n",
    "3. **Backward Propagation**: The error is then propagated back through the network, from the output layer to the input layer. The gradient of the loss function with respect to each weight and bias in the network is calculated along the way. This gradient indicates how much the loss would change if the corresponding weight or bias were changed by a small amount.\n",
    "\n",
    "4. **Update Weights and Biases**: The weights and biases in the network are then updated in the opposite direction of the gradients. This is done to decrease the loss. The size of the updates is determined by the learning rate, which is a hyperparameter that controls how fast the network learns.\n",
    "\n",
    "5. **Iterate**: This process is repeated for a number of iterations (or epochs). With each iteration, the weights and biases in the network are adjusted to gradually decrease the loss, and the network becomes better at predicting the output for given inputs.\n",
    "\n",
    "Here's a simplified example of how you might implement Gradient Descent in Python:\n",
    "\n",
    "```python\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        current_output = inputs\n",
    "        for layer in self.layers:\n",
    "            current_output = layer.forward_propagation(current_output)\n",
    "        return current_output\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return np.mean(np.square(y_true - y_pred))  # Mean Squared Error loss\n",
    "\n",
    "    def back_propagation(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        for layer in reversed(self.layers):\n",
    "            error = layer.back_propagation(error)\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= learning_rate * layer.d_weights\n",
    "            layer.biases -= learning_rate * layer.d_biases\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            y_pred = self.forward_propagation(X)\n",
    "            self.back_propagation(y, y_pred)\n",
    "            self.update_weights(learning_rate)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.biases = np.random.rand(output_size)\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.outputs\n",
    "\n",
    "    def back_propagation(self, error):\n",
    "        self.d_weights = np.dot(self.inputs.T, error)\n",
    "        self.d_biases = np.sum(error, axis=0, keepdims=True)\n",
    "        return np.dot(error, self.weights.T)\n",
    "```\n",
    "\n",
    "In this code, `NeuralNetwork` is a class representing an ANN, and `Layer` is a class representing a layer in the network. The `forward_propagation` method in the `NeuralNetwork` class passes the input data through each layer in the network, and the `forward_propagation` method in the `Layer` class calculates the output of the layer given some inputs. The `back_propagation` method in the `NeuralNetwork` class propagates the error back through the network, and the `back_propagation` method in the `Layer` class calculates the gradients of the weights and biases in the layer. The `update_weights` method in the `NeuralNetwork` class updates the weights and biases in each layer according to the gradients and the learning rate. The `train` method in the `NeuralNetwork` class performs the training process for a given number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c75fe4-6fd9-4242-9c17-44a7cf851b85",
   "metadata": {},
   "source": [
    "# Batches and Epoch\n",
    "\n",
    "In the context of training an Artificial Neural Network (ANN), \"Batches\" and \"Epochs\" are terms used to describe the way the training data is passed through the network.\n",
    "\n",
    "1. **Batch**: A batch is a subset of the training data. Instead of passing the entire training dataset through the network at once (which can be computationally expensive), the data is divided into smaller batches. Each batch is passed through the network independently. The size of these batches is a hyperparameter that you can tune. Smaller batch sizes can lead to a more accurate estimation of the gradient, but the process can be slower due to the overhead of loading more batches. Larger batch sizes can lead to faster training, but the estimation of the gradient can be less accurate.\n",
    "\n",
    "2. **Epoch**: An epoch is a single pass through the entire training dataset, once for each batch. For example, if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete one epoch. The number of epochs is a hyperparameter that determines how many times the learning algorithm will work through the entire training dataset. Training a neural network for too many epochs can lead to overfitting, and training for too few epochs may mean the network is underfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253b1ca-0e5f-4ed3-8573-1ee6bb8603a0",
   "metadata": {},
   "source": [
    "# Popular Open-source NN Models\n",
    "\n",
    "## Popular NN Frameworks \n",
    "\n",
    "1. **TensorFlow**: TensorFlow is an end-to-end open-source platform for machine learning developed by Google. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.\n",
    "\n",
    "2. **Keras**: Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.\n",
    "\n",
    "3. **PyTorch**: PyTorch is an open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab.\n",
    "\n",
    "4. **Caffe (Convolutional Architecture for Fast Feature Embedding)**: Caffe is a deep learning framework that allows users to create artificial neural networks (ANNs) on a leveled architecture. It is developed by the Berkeley Vision and Learning Center (BVLC).\n",
    "\n",
    "5. **Theano**: Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It is widely used for unit-testing and self-verification to detect and diagnose many types of errors.\n",
    "\n",
    "6. **Torch**: Torch is an open-source machine learning library, a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep learning.\n",
    "\n",
    "7. **MXNet**: MXNet is a deep learning framework that allows you to define, train, and deploy deep neural networks on a wide array of devices, from cloud infrastructure to mobile devices. It is highly scalable and supports a flexible programming model and multiple languages.\n",
    "\n",
    "8. **CNTK (Microsoft Cognitive Toolkit)**: CNTK describes neural networks as a series of computational steps via a directed graph. It allows users to easily realize and combine popular model types such as feed-forward DNNs, convolutional neural networks (CNNs), and recurrent neural networks (RNNs/LSTMs).\n",
    "\n",
    "9. **PaddlePaddle**: PaddlePaddle (PArallel Distributed Deep LEarning) is an easy-to-use, efficient, flexible, and scalable deep learning platform, which is originally developed by Baidu scientists and engineers for the purpose of applying deep learning to many products at Baidu.\n",
    "\n",
    "10. **Chainer**: Chainer is a Python-based deep learning framework aiming at flexibility. It provides automatic differentiation APIs based on the define-by-run approach (a.k.a. dynamic computational graphs) as well as object-oriented high-level APIs to build and train neural networks.\n",
    "\n",
    "## Popular NN Models\n",
    "\n",
    "Here are some popular open-source pre-trained Neural Network models that you can use in your own projects:\n",
    "\n",
    "1. **VGG16 and VGG19**: These are models used to win ILSVR (ImageNet) competition in 2014. They are very effective at extracting features from images.\n",
    "\n",
    "2. **InceptionV3**: This model was a part of Google's submission for ILSVR in 2015. It uses a lot of computational power but is very effective at classifying images.\n",
    "\n",
    "3. **ResNet50**: This model, developed by Microsoft, introduced the concept of \"skip connections\" to mitigate the problem of vanishing gradients in deep networks. It won the ILSVR competition in 2015.\n",
    "\n",
    "4. **Xception**: This model is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.\n",
    "\n",
    "5. **MobileNet**: Developed by Google, MobileNet is a model designed for mobile and embedded vision applications. It is based on a streamlined architecture that uses depthwise separable convolutions to build lightweight deep neural networks.\n",
    "\n",
    "6. **EfficientNet**: EfficientNet, also developed by Google, uses a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient.\n",
    "\n",
    "7. **YOLO (You Only Look Once)**: This is a real-time object detection system that is extremely fast and accurate.\n",
    "\n",
    "8. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
    "\n",
    "9. **GPT (Generative Pretrained Transformer)**: Developed by OpenAI, GPT is a large transformer-based language model with 175 billion parameters.\n",
    "\n",
    "10. **Transformer Models (like T5, RoBERTa, DistilBERT, etc.)**: These are variations and improvements over the original Transformer model for NLP tasks, developed by various organizations.\n",
    "\n",
    "These models are available in machine learning libraries like TensorFlow and PyTorch, and can be used directly with a few lines of code. They are trained on large datasets and can be fine-tuned on a specific task to achieve high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7b21a-1832-4e54-bfb8-e98c0c6396ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
